# 数据库

### 数据库设置冗余字段：

数据库设置冗余字段：对于不会频繁更改，但会频繁查询的字段可以设置冗余。

例子：有学生表 student(id, sno, name, major_id) 和 专业信息表 major(id, major_name)。现有如下查询：select s.id, sno, name, major_name from student as s join major as m on s.major_id = m.id;

可以看到为了查到 major_id 对应的专业名称，需要将两张表连接。这时候可以设计冗余，使得学生表变为 student(id, sno, name, major_id, major_name)

### 数据库的三范式：

**第一范式**：**（属性不可分）** 必须不包含重复组的关系，即每一列都是不可拆分的原子项。

<img src="https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202112202113311.png" alt="image-20210516232740073" style="zoom:50%;" />

**第二范式**：**（属性完全依赖主键，消除部分子函数依赖）** 首先满足第一范式的条件，并且所有非主属性都完全依赖于主属性。**符合第二范式的关系模型可能还存在数据冗余、更新异常等问题。**

**第三范式**：**（属性不依赖于其他非主属性，消除传递依赖）** 首先满足第二范式的条件，并且所有非主属性对任何候选关键字不存在传递依赖。即每个属性跟主键有直接关系而不是间接关系，像 a --> b --> c 这种就是间接关系。

**第二范式和第三范式的区别详见：https://blog.csdn.net/Dream_angel_Z/article/details/45175621**



### MyISAM 和 InnoDB 的区别：

MyISAM 性能不错，而且提供了例如全文索引，压缩，空间函数这样的特性。但是它不支持事务和行级锁，最大缺陷是奔溃后无法安全恢复。

|         | MyISAM | InnoDB                                          |
| ------- | ------ | ----------------------------------------------- |
| 锁       | 表级     | 行级 + 表级                                         |
| 事务和奔溃恢复 | 不支持    | 提供事务和外键，具有事务、回滚和奔溃恢复的能力                         |
| MVCC    | 不支持    | 应对高并发事务，MVCC比单纯加锁更加有效。MVCC仅在读已提交和可重复读两个隔离级别下工作。 |

1. InnoDB 支持事务，MyISAM 不支持
2. InnoDB 支持外键，MyISAM 不支持
3. InnoDB 是聚簇索引，MyISAM 是非聚簇索引
4. InnoDB 不保存表的具体行数，执行 select count(*) from table 时需要全表扫描（这和 InnoDB 的事务特性有关，对于不同的事务，他们的行数是不同的）。MyISAM 通过一个变量保存了整个表的行数。
5. MyISAM 支持全文索引，压缩，空间函数这样的特性，InnoDB 并不支持。
6. InnoDB 支持表、行级锁，MyISAM 仅支持表级锁。
7. InnoDB 必须有唯一索引（如主键），如果用户没有设置的话，会自动产生一个隐藏列 row_id 来充当。MyISAM 并不需要。

### MySQL 选择 B+ 树做索引的原因？

...

### 事务的四大特性？ ACID

原子性：事务是最小的执行单位，不允许分割。确保动作要么全完成，要么全不起作用。

一致性：执行事务前后，数据保持一致，多个事务对同一数据的读取结果应该相同。

隔离性：并发访问数据库时，一个事务的执行不能被其他事务干扰。

持久性：一个事务提交后，它对数据库中数据的改变是持久的，即使数据库发生故障也不应该有影响。

### MySQL 事务的实现：

**原子性的实现**：要保证原子性就需要在异常发生时，对已经执行的操作进行回滚。 MySQL 的恢复机制是通过回滚日志（undo log）实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后再对数据库中对应行写入。

**持久性的实现**：使用重做日志（redo log）实现。重做日志包含两部分，一是内存中的重做日志缓冲区，二是磁盘上的重做日志文件。当一个事务尝试对数据进行修改时：1.会先将数据从磁盘读入内存，并更新内存中缓存的数据。2.然后生成一条重做日志写入重做日志缓存。3.事务真正提交时，MySQL 将重做日志缓存中内容刷新到重做日志文件中，再将内存中的数据更新到磁盘。

发生错误，在数据库重启时，会从重做日志中找出未被更新的日志重新执行以满足持久性。

**一致性的实现**：保证原子性，隔离性，持久性才能做到事务的一致性。

### MySQL 事务的实现（Open-ATA）：

- 通过 redo、undo 日志文件实现。
- redo 和 undo 都有一个在内存中的缓存，叫做 redo_buf 和 undo_buf（同样 data 也有 data_buf）

#### undo 实现事务的过程（undo 记录旧的状态）

1. 初始值 A = 3，B = 5 ，要修改 A = 33， B = 55
2. 事务开始
3. 记录 A = 3 到 undo_buf
4. 修改 A = 33 到 data_buf
5. 记录 B = 5 到 undo_buf
6. 修改 B = 55 到data_buf
7. 将 undo_buf 写入磁盘的 undo 日志文件
8. 将 data_buf 写入磁盘的 datafile 数据文件
9. 提交事务

#### 奔溃处理

- undo_buf 写入磁盘之前奔溃：
  - 由于数据在内存中未写入磁盘，磁盘数据未被毁坏。
- data_buf 写入磁盘之前奔溃：
  - 根据 undo 日志文件（记录初始状态）恢复初始状态。
- 事务提交之前必须写入磁盘，磁盘 IO 会影响性能。

#### redo + undo 实现事务的过程 （redo 记录新的状态）

1. 初始值 A = 3，B = 5 ，要修改 A = 33， B = 55
2. 事务开始
3. 记录 A = 3 到 undo_buf
4. 记录 A = 33 到 redo_buf
5. 记录 B = 5 到 undo_buf
6. 记录 B = 55 到 redo_buf
7. 将 redo_buf 写入磁盘的 redo 日志文件
8. 提交事务

#### 奔溃处理

- redo_buf 写入磁盘之前奔溃：
  - 数据重新从磁盘加载即可，未被破坏。
- redo_buf 写入磁盘之后奔溃：
  - 通过 redo 来保证重新写入。
- 提交事务之前奔溃：
  - 通过 undo 来恢复数据。

### 关于 MVCC

- MVCC ： Multi-Version Currency Control，多版本并发控制
- 能解决幻读的问题

#### MVCC 实现机制：每行增加三个隐藏字段

- DB_TRX_ID: 事务ID，每处理 一个事务，ID + 1
  - 记录事务创建版本号 or 事务删除版本号
  - SELECT 时读取满足如下规则的行：
    - 事务版本号**小于等于当前事务版本号**的记录
    - 为确保读到的行在事务开始之前没有被删除，行的删除要么未定义，要么大于当前事务的版本号。
    - 新增的行的事务 ID 肯定大于当前事务的 ID，也读不到。
  - INSERT 时创建时版本 = 事务 ID
  - DELETE 时删除时版本 = 事务 ID
  - UPDATE 时复制新数据的创建时版本 = 事务 ID，旧数据的删除版本号 = 事务 ID
- DB_ROLL_PTR:回滚指针，指向回滚段在 undo_log 日志中的位置。
- DB_ROW_ID:行ID，MySQL 内部生成，若无主键和唯一索引，就会利用该字段建立聚簇索引，无其他用。

**MVCC 中同一份数据会保存多个版本，是通过 undo log 实现的 ----> 即旧数据保存在 undo log 中，新版本会记录旧版本的回滚指针。**

### 并发事务带来的问题：

**脏读**：一事务访问数据且修改但未提交，二事务读取未修改的数据，这就是脏读（依据脏读数据做的操作可能不正确）。

**丢失修改**：一事务读取，二事务读取，此时两事务读取到的值应当相同。一事务修改，二事务再修改。这样就丢失了一事务的修改。

**不可重复读**：一事务读取，二事务读取并修改，一事务读取。这样事务一两次读取到的数据不一样。

**幻读**：一事务读取几行数据，二事务插入数据，一事务再次读取发现多出了几行数据。

### 四个隔离级别及实现原理：

|      | 脏读  | 不可重复读 | 幻读  |
| ---- | --- | ----- | --- |
| 读未提交 | √   | √     | √   |
| 读已提交 | X   | √     | √   |
| 可重复读 | X   | X     | √   |
| 串行化  | X   | X     | X   |

读未提交：全程不加锁。事务中对数据库进行变更的语句在执行过后，不需提交便可以被别的事物读取到。

读已提交：一个事务只能读到其他事务已经提交过的数据（其他事务 commit 之后的数据）。

可重复读：MVCC

#### MySQL 如何实现可重复读？

MVCC 多版本并发控制是 MySQL 基于乐观锁理论实现隔离级别的方式，用于实现读已提交和可重复读隔离级别。

#### MVCC 是否能解决幻读？

对于**仅触发快照读的场景下，能够解决。**但是**若触发当前读，则不行。**

例：1、事务一读，事务二插入并提交，事务一再读，此时事务一仅触发快照读，是不会出现幻读现象的。

2、事务一读，事务二插入并提交，事务一进行 insert、update、delete 操作都会触发当前读，此时事务进行读操作时，会出现幻读现象。

#### 如何解决幻读？

在快照读的情况下，MySQL 使用 MVCC 来避免幻读。

在当前读的情况下，MySQL 通过加写锁或 next-key lock 来避免其他事务修改（当然串行化也可以）。

### MySQL 中的锁：

**行级锁**：MySQL 中粒度最小的锁，只针对当前操作的行加锁。行级锁能大大减少数据库操作的冲突。加锁粒度小，并发度高，加锁开销大，加锁慢。

**表级锁**：MySQL 中粒度最大的锁，针对当前操作的表进行加锁。实现简单，资源消耗少，加锁快。加锁粒度大，触发锁冲突概率高，并发度最低。

**记录锁**：

**间隙锁**：

**临键锁**：

### 在什么情况下会只触发间隙锁？

索引上的等值查询，向右遍历且最后一个值不满足等值条件的时候， 原本的 next-key lock 会退化为**间隙锁**。

如：假设有表 t，仅包含一个字段 id，并且 id 为主键，表 t 中有数据 5， 10， 15， 20。做如下查询：

```mysql
SELECT id FROM t WHERE id = 12;
```

此时会锁区间 (10, 15) ，不能插入 10 和 15 是因为 duplicate key ，而不是因为锁。

再比如：假设有表 t，仅包含一个字段 id，并且 id 为普通索引，表 t 中有数据 5， 10， 15， 20。做如下查询：

```mysql
SELECT id FROM t WHERE id = 12;
```

此时能够更新 id = 10 或者 id = 15 的值，说明 10 和 15 未被加锁，锁的是区间 (10, 15) 。

### 通过 MySQL 的事务和锁防止超卖

基于 MySQL 的事务和锁实现方式：

1. 开启事务
2. 查询库存，并显示的设置写锁（排他锁）：SELECT * FROM table_name WHERE ... FOR UPDATE
3. 生成订单
4. 去库存，隐示的设置写锁（排他锁）：UPDATE goods SET counts = counts - 1 WHERE id = ? 
5. commit，释放锁

如果不开启事务，第 2 步即使加锁，第一个会话读取库存结束后，就会释放锁，第二个会话仍有机会在去库存前读库存，出现超卖。

如果开启事务，第 2 步不加锁，第一个会话读库存结束后，第二个会话可能出现 `脏读`，出现超卖。

既加事务，又加读锁：开启事务，第一个会话读库存时加读锁，并发时，第二个会话也允许获得读库存的读锁，但是第一个会话执行写操作时，写操作会等待第二个会话的读锁，第二个会话执行写操作时，操作也会等待第一个会话的读锁，会出现死锁。

既加事务，又加写锁：第一个会话读库存时加写锁。写锁会阻止其他事务的读锁和写锁。直到 commit 才会释放，此时才允许第二个会话查询库存，不会出现超卖现象。

Notice ：在事务中，锁只有在执行 commit 或者 rollback 的时候才会释放， 并且所有的锁都在**同一时刻**释放。

### 使用 MySQL 实现的乐观锁：

一般有两种方式：

1. 使用数据版本（version）记录机制实现，即为数据增加一个版本表示（就是在数据表中增加一个数字类型的 version 字段）。当读取数据的时候，将 version 字段一同读出，数据每更新一次，就对 version 值加一。当提交更新的时候，判断数据表对应记录的当前版本信息和第一次取出来的 version 值进行比对，如果两者相同，则予以更新。不同的话，则认为是过期数据，放弃更改。
2. 其实和第一种实现方式差不多，同样是在数据表中添加一个字段，名称无所谓，字段类型使用**时间戳**。实现方式和第一种相同。

利用 MySQL 实现的乐观锁，实现下单的操作：

1. 查询商品信息： 
   
   ```mysql
   SELECT (status, stock_count, version) FROM goods WHERE id = #{id}
   ```

2. 根据商品信息生成订单

3. 修改商品 status 为 2：
   
   ```mysql
   UPDATE goods SET status = 2, version = version + 1
   WHERE id = #{id} AND  version = #{version}
   ```

如果上面的更新操作失败了，则放弃生成的订单。

### 聚簇索引和非聚簇索引

**聚簇索引**：将数据存储与索引放到一起，找到索引也就找到了数据。（一张表只允许有一个）

**非聚簇索引**：将数据与索引分开，索引的叶子节点指向了数据的对应行。在访问数据时要先搜索索引，在通过索引在磁盘中找到相应的数据。

### 哪些字段适合建立索引：

1. 表的主键，外键必须有索引。
2. 经常与其他表进行连接的表，在连接字段上应该有索引。
3. 经常出现在 where 子句中的字段。
4. 查询频率高的字段。
5. 尽量使用单字段索引。
6. 应该选择小字段。大的文本字段，超长字段不要建立索引。

### 什么是最左匹配原则？

在 MySQL 建立联合索引时会遵循最左匹配原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。对于 建立 (a, b, c) 这样的联合索引，实际上产生了 a 索引， a, b 索引， a, b, c 索引。

### 索引失效的情况：

1. 违反了最左匹配原则。
2. 在索引列上用了函数或手动进行了类型转换。
3. 使用了 !=, < , > 
4. like 后以通配符开头 （ %abc ）

### 索引的优点和缺点

索引的优点主要体现在以下几个方面：

1. 索引可以减少服务器需要扫描的数据量，从而大大提高查询的效率。
2. 唯一索引能保证表中数据的唯一性。
3. 利用索引对数据存储的特性，可以使查询语句避免排序和创建临时表。
4. 加速表与表之间的连接速度。
5. 显著减少查询中分组和排序的时间。

索引的缺点主要体现在以下几个方面：

1. 索引的创建和维护，需要时间，并且数据量越大时间越长，这会造成工作量的增加。
2. 索引会造成数据量大增加，除了数据表中数据占据空间外，每一个索引还需要占一定的物理空间。
3. 对表中的数据进行增加，修改，删除的时候，索引也要同时进行维护，降低了数据的维护速度。

### MySQL 中的索引类型

普通索引，唯一索引，主键索引，组合索引，全文索引。

### MySQL 中的索引：

1. 普通索引：最基本的索引，没有任何限制，仅用来加速查询。
2. 唯一索引：在普通索引的基础上，加上索引列的值必须唯一的限制（但是允许空值），如果是组合索引的话，则列值的组合必须唯一。
3. 主键索引：一种特殊的唯一索引，一个表只能有一个主键，主键不允许有空值，一个表也只能有一个主键索引。
4. 组合索引：在多个字段上创建的索引，只有在查询条件中使用了组合索引的第一个字段，索引才会被使用。使用组合索引遵循最左前缀原则。
5. 全文索引：只能在 char，varchar，text 列上创建全文索引。全文索引主要用来查找文本中的关键字，并且这种方式实现的查询过滤比使用 like + % 快 N 倍。

ps.

覆盖索引： SELECT 后的数据列只用在索引中就能全部获得，所以不必读取数据行。换句话说，就是查询的列要被索引覆盖。

### MySQL 主从架构

**概念：**通过配置多台数据库的主从关系，可以实现将一台数据库服务器的数据更新同步到另一台服务器上，**实现数据库的读写分离，从而改善数据库的负载压力。**

**MySQL 主从复制主流架构模型：**1、一主一从 2、多主一从 3、双主复制 4、级联复制

**MySQL 主从复制原理：** 

MySQL 主从复制涉及三个线程：一个在主节点的线程： ``log dunmp thread``  从库会生成两个线程：一个 I/O 线程，一个 SQL 线程。如下图所示：

![4](https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202201242242254.jpg)

主库会生成一个 log dump 线程，用来给从库 I/O 线程传 binlog 数据。从库 I/O 线程会去请求主库的 binlog ，并将得到的 binlog 写到本地的 relay log（中继日志）文件中。SQL 线程会读取 relay log 文件中的日志，并解析成 SQL 语句逐一执行。

### 如何避免mysql双主架构出现会循环的数据更新？

问题描述：如果主库触发SQL语句：insert into test_data(name) values(‘aa’);

那么Master1生成binlog,推送数据变化到Master2,在Master2上面生成relay log,然后交由sql thread进行变更重放。如果Master2消费了relay的数据，然后会产生binlog(log_slave_updates默认开启），这个时候产生的binlog会继续推送到Master1消费，然后来来回回推送，一套insert语句就无穷无尽了。

**解决该问题的关键部分：Master 2 是否推送了先前的 binlog 到 Master 1 ？**

1、如果推送了，Master 1 是如何过滤，避免后续无限循环？

2、如果没有推送，Master 2 是如何过滤的？

**解决方法：** 1、Master 1 更新数据时，在事务开始前产生一个分布式事务 ID，一同记录到 binlog 日志中。

2、Master 2 对 I/O 线程将变更的 binlog 写入本地的 relay log 中。

3、Master 2 的 SQL 线程从 relay log 中获取分布式事务 ID，然后对比 Master 2 端的 binlog 是否有记录。如果有记录，则说明该事务已经执行过了，Master 2 忽略。

### 数据库主从不一致怎么办？

    1.常见的数据库架构是怎样的？

    一主多从，主从同步，读写分离。

    2.为什么会出现不一致？

    主从同步有时延，这个时延期间读从库，可能读到不一致的数据。

        （1）服务发起一个写请求。

        （2）服务发起读请求，此时同步未完成，读到一个脏数据。

        （3）数据库主从同步最后才完成。

    3.如何避免主从延时导致的不一致？

    （1）忽略：绝大部分业务能够接受短时间的不一致。如果业务能接受，则最推荐此法。

    （2）强制读主：One. 使用一个高可用主库提供数据库服务 Two.读和写落到主库上 Three.采用缓存来提升系统读的性能。

    （3）选择性读主：利用缓存记录必须读主的数据

        当写请求发生时，One.写主库 Two.将哪个库，那个表，哪个主键三个信息拼装成 key 设置到缓存中，超时时间设置为同步时延。

        当读请求发生时，One.根据库，表，主键拼装 key。Two.判断 Cache 中是否包含该 key，包含则从数据库读，不包含则从缓存或从库读。

### Redis 一般用来做什么：

Redis 是内存高速缓存数据库，在服务器中常用来存储需要频繁调取的数据，这样可以大大节省系统直接读取磁盘数据的 IO 开销，更重要的是可以极大提升速度。

比如某网站一天有 100 万人访问，其中某个板块需要从数据库查询，那么一天就要多消耗 100 万次数据库请求。这种情况我们可以把热点数据存到 Redis 中，需要的时候，直接从内存中获取，极大的提高了速度，节省服务器开销。

### Redis 中的底层数据结构：

#### 简单动态字符串（SDS）

1、定义：

```c
struct sdshdr{
  //记录 buf 数组中已使用字节的数量
  //等于 SDS 所保存字符串的长度
  int len;

  //记录 buf 数组中未使用字节的数量
  int free;

  //字节数组，用于保存字符串
  char buf[];
}
```

实例图：

<img src="https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202112062133993.png" alt="image-20211206213112778" style="zoom:50%;" />

2、SDS 与 C 字符串的区别

- 常数复杂度获取字符串长度

由于 SDS 中 len 属性的存在， SDS 获取字符串长度的时间复杂度为 O(1)。由于 C 字符串需要遍历整个字符串，对遇到的每个字符串计数，所以时间复杂度为 O(n).

- 杜绝缓冲区溢出

SDS 在进行拼接操作时，会先检查 SDS 空间是否足够，不够的话会先扩展 SDS 的空间。

- 减少修改字符串带来的内存重分配次数

对于 C 语言，如执行拼接操作，操作前需要先执行内存扩展，如果忘记可能会导致缓冲区溢出。如执行截断操作，操作后需要释放，如果忘记可能产生内存泄漏。

如果修改字符串不常出现，则以上方式可以接受，但在 Redis 中是不行的。在 SDS 中，**通过未使用空间解除了字符串长度和底层数目长度之间的关联。** 基于此有两种优化策略：1、空间预分配 2、惰性空间释放

**空间预分配**：（1）SDS 的长度（即 len < 1MB) : 假设扩容后 len = 13，则 SDS 的 buf 数组实际长 13 + 13 + 1 = 27

（2）SDS 的长度（即 len >= 1MB），则每次扩容 1MB。假设扩容后 len = 30MB，则 SDS buf 数组实际长 30MB + 1MB + 1byte

**惰性空间释放**：收缩的时候，空间不是立即回收的。

- 二进制安全

二进制数据中可能有空字符，这就限制了 C 字符串只能保存文本数据。

- 兼容部分 C 字符串函数

#### 链表

链表节点的实现：

```c
typedef struct listNode{
  //前置节点
  struct listNode *prev;

  //后置节点
  struct listNode *next;

  //节点的值
  void *value;
}
```

<img src="https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202112062150442.png" alt="image-20211206215024399" style="zoom:50%;" />

使用 list 来持有链表，操作会更方便，list 定义如下：

```c
typedef struct list{
  //表头节点
  listNode *head;

  //表尾节点
  listNode *tail;

  //链表所包含的节点数量
  unsigned long len;

  //节点值复制函数
  void *(*dup)(void *ptr);

  //节点值释放函数
  void *(*free)(void *ptr);

  //节点值对比函数
  int (*match)(void *ptr, void *key);
}
```

下图是由 list 和 listNode 构成的链表：

<img src="https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202112062155025.png" alt="image-20211206215540970" style="zoom:50%;" />

#### 字典

**哈希表结构定义：**

```c
typedef struct dictht{
  //哈希表数组
  dictEntry **table;

  //哈希表大小
  unsigned long size;

  //哈希表大小掩码，用于计算索引值
  //总是等于 size - 1
  unsigned long sizemask;

  //该哈希表已有节点的数量
  unsigned long used;
} dictht;
```

table 属性是数组，指向 dictEntry 结构，size = table 数组大小，used 记录哈希表目前已有键值对的数量。

**哈希表节点结构定义：**

```c
typedef struct dictEntry{
  //键
  void *key;

  //值
  union {
    void *val;
    uint64_tu64;
    int64_ts64;
  } v;

  //指向下一个哈希表节点，形成链表
  struct dictEntry *next;
} dictEntry;
```

key 键， v 值，next 用于解决 Hash 冲突

哈希表和哈希表节点一起构成的数据结构实例：

<img src="https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202112062343782.png" alt="image-20211206234326723" style="zoom:50%;" />

**Redis 中字典的定义如下：**

```c
typedef struct dict{
  //类型特定函数
  dictType *type;

  //私有数据
  void *privdata;

  //哈希表
  dictht ht[2];

  // rehash 索引
  //当 rehash 不在进行时，值为 -1
  int trehashidx;
} dict;
```

ht[2] 一般只会用到 ht[0]，ht[1] 只在对 ht[0] 进行 rehash 时使用，rehashidx 记录 rehash 的进度，若没有进行 rehash 则为 -1。

下图是普通状态下（没有进行 rehash ）的字典：

<img src="https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202112062352841.png" alt="image-20211206235220805" style="zoom:50%;" />

**解决键冲突的方法：**链地址，头插。

**rehash 的步骤和时机：**

步骤：1. 扩容或搜索 （为 ht[1] 分配空间）

            2. ht[0]  ----> rehash ----> ht[1]
               3. ht[1] 置为 ht[0]

扩展时机：loadFator >= 1 (未执行 BGSAVE，BGREWRITEAOF) 或者 loadFator >= 5 (在执行 BGSAVE，BGREWRITEAOF) 

收缩时机：loadFator <= 0.1

**渐进式 Hash：**避免 rehash 对服务器性能造成影响，服务器不是将 ht[0] 一次性 rehash 至 ht[1]，而是分批次进行。

1. 为 ht[1] 分配空间
2. 将 rehashidx 的值置为 0，表示 rehash 正式开始。
3. 在这期间，每次对字典的增删改查，除完成指令外，还会将 rehashidx 对应的所有键值对 rehash 到 ht[1]
4. 直至全部迁移完成， rehashidx = -1

#### 跳跃表

跳跃表是一种有序的数据结构，通过每个节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。支持平均 O(logN),最坏 O(N) 复杂度的节点查找，还可以通过顺序操作来批量处理节点。

**跳跃表节点的定义：**

```c
typedef struct zskiplistNode{
  //层
  //每次创建一个心跳跃表节点的时候，程序会根据幂次定律（越大的数出现的概率越小）随机生成一个介于 1 和 32 之
  //间的值作为 level 数组的大小。
  struct zskiplistLevel{
    //前进指针
    struct zskiplistNode *forward;

    //跨度
    unsigned int span;
  } level[];

  //后退指针
  struct zskiplistNode *backward;

  //分值
  double score;

  //成员对象
  robj *obj;
} zskiplistNode;
```

**跳跃表结构定义：**

```c
typedef struct zskiplist{
  //表头节点和表尾节点
  struct zskiplistNode *header, *tail;

  //表中节点的数量
  unsigned long length;

  //表中层数最大的节点的层数
  int level;
} zskiplist;
```

下图是由多个跳跃表组成的跳跃表：

<img src="https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202112072254296.png" alt="image-20211207225418096" style="zoom:50%;" />

下图是带有 zskiplist 结构的跳跃表：

<img src="https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202112072255310.png" alt="image-20211207225527270" style="zoom:50%;" />

#### 整数集合

当集合只包含整数值元素，并且元素数量不多，就会使用整数集合作为集合的底层实现。

**整数集合结构定义：**

```c
typedef struct intset{
  //编码方式 （有三种 ： 16， 32， 64）
  uint32_t encoding;

  //包含的元素数量
  uint32_t length;

  //保存元素的数组
  int8_t contents[];
} intset;
```

下图是一个包含 5 个 int16_t 类型整数数值的整数集合：

<img src="https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202112072302899.png" alt="image-20211207230253833" style="zoom:50%;" />

以上图为例，如果这时候插入一个 -2675256175807981027 （16 位 编码范围表示不了，会引起升级）

**升级过程：**1.根据新元素类型，扩展整数集合底层数组大小，并为新元素分配空间。

​                   2.将现有元素转换成新类型，保持有序性放到正确的位置上。

​                   3.将新元素添加到底层数组里（**要么是第 0 位，要么是最后一位**）

**升级的好处：**1.提升灵活性 2.节约内存

**降级：**整数集合不支持降级，一旦升级，就会保持升级后的编码状态。

#### 压缩列表

压缩列表是列表或哈希的底层实现之一。

**构成：**压缩列表是 Redis 为了节约内存而开发的，是由一系列特殊编码的连续内存块组成的顺序型数据结构。构成如下：

```
 +---------+--------+-------+--------+--------+--------+--------+-------+
 | zlbytes | zltail | zllen | entry1 | entry2 |  ...   | entryN | zlend |
 +---------+--------+-------+--------+--------+--------+--------+-------+
```

zlbytes : 记录压缩列表总长（占用字节数）

zltail : 记录表尾节点距离压缩列表起始地址多少字节

zllen : 记录压缩列表包含的元素个数

zlend : 特殊值 0xFF,标记压缩列表末端

**压缩列表节点的构成：**

```
area        |<------------------- entry -------------------->|
            +------------------+----------+--------+---------+
component   | pre_entry_length | encoding | length | content |
            +------------------+----------+--------+---------+
```

pre_entry_length : **可以根据该值从当前节点计算出前一节点的地址，**记录前一个节点的长度，如果前一个节点的长度小于 254 字节，则该属性长 1 字节。若长度大于等于 254 字节，则该属性长度为 5 字节。

encoding : **可以根据该值从当前节点计算出下一节点的地址，**记录了节点 content 属性所保存的数据及长度。

content : 负责保存节点的值，值可以是字节数组或整数。类型和长度由 encoding 属性决定。

**连锁更新问题**

### Redis 中的五种基本数据结构

Redis 中有五种基础数据结构，分别是：string（字符串，存储 K-V 键值对），list（列表，消息队列，朋友圈的点赞列表），hash（字典，存储对象），set（集合，共同关注、共同好友等），sorted set（有序集合，排行榜）。

#### 实现：

| 类型   | 实现方式                                             |
| ------ | ---------------------------------------------------- |
| string | 整数集合，smbstr编码的简单动态字符串，简单动态字符串 |
| list   | 压缩列表，双端列表                                   |
| hash   | 压缩列表，字典                                       |
| set    | 整数集合，字典                                       |
| zset   | 压缩列表，跳表                                       |

![image-20220206122450883](https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202202061224056.png)

### Redis 持久化机制

Redis 支持持久化，并且有两种方式实现持久化：一种是快照，一种是只追加文件。

**快照（RDB快照）**：通过创建快照来获得存储在内存里的数据在某个时间点的副本。

**只追加文件（AOF）**：每执行一条会更改 Redis 中数据的命令， Redis 都会把该命令写入硬盘中的 AOF 文件。

### 缓存穿透、缓存雪崩、缓存击穿

**缓存穿透**：大量请求缓存和数据库中没有的数据。

解决方案：1.做好参数校验，不合法的参数请求直接抛出异常信息，返回客户端。2.布隆过滤器

**缓存雪崩**：缓存在同一时间大面积失效，导致之后的请求全部落在数据库上。

解决方案：1.采用 Redis 集群，避免单机出现问题，整个缓存服务都失效。2.限流，避免同时处理大量的请求。3.批量向 Redis 中存储数据的时候，把每个 Key 的失效时间都加上一个随机值。

**缓存击穿**：某个 Key 非常热点，大并发集中对这个点进行访问，当这个点的缓存失效，大量请求落在数据库。

解决方案：设置热点数据永不过期。

一般避免以上情况发生我们从三个时间段去分析下：

- 事前：**Redis** 高可用，主从+哨兵，**Redis cluster**，避免全盘崩溃。
- 事中：本地 **ehcache** 缓存 + **Hystrix** 限流+降级，避免 **MySQL** 被打死。
- 事后：**Redis** 持久化 **RDB**+**AOF**，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。

还可以设置每秒的请求，有多少能通过组件，剩余的未通过的请求，怎么办？**走降级**！可以返回一些默认的值，或者友情提示，或者空白的值。

### Redis 集群方案

**主从复制：**典型的读写分离。

**哨兵 sentinel：**能够实现自动的故障转移。在 Master 节点故障时，可选举 slave 节点为新的 Master 节点，并且其余节点开始从新的主节点复制数据。

选举方案：1、过滤故障节点 2、根据优先级选择，配置文件 slave-priority 的信息，最低值的成为主节点。 3、选择复制偏移量最大（同步的数据量最多）的从节点为主。

<img src="https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202201251723183.png" alt="image-20220125172304096" style="zoom:30%;" />

**Redis 内置集群 cluster：**使用 Redis slot 算法。

### Redis 分布式寻址算法：

1. Hash 算法 （问题：大量缓存重建，缓存雪崩）
   
   来一个 Key，首先计算 Hash 值，然后对节点数进行取模。然后加载到不同的 master 节点上。一旦某个 master 节点宕机，之后的请求，都会根据剩余 master 节点数去取模，尝试去取数据。 这会导致大部分的请求过来，全部无法拿到有效的缓存，导致大量的流量涌入数据库。

2. 一致性 Hash 算法
   
   **优点**：可扩展性，一致性 Hash 算法保证了增加或减少服务器时，数据存储的改变量最少，相比传统 Hash 算法大大节省了数据移动的开销。
   
   **一致性 Hash 算法与 Hash 算法的关系**：一致性 Hash 算法是在 Hash 算法基础上提出的，在动态变化的分布式环境中，Hash 算法应该满足的几个条件：平衡性、单调性和分散性。
   
   平衡性：是指Hash的结果应该平均分配到各个节点，这样从算法上解决了负载均衡的问题。
   
   单调性：是指在新增或者删除节点时，不影响系统的正常运行。
   
   分散性：是指数据应该分散地存放在分布式集群中的各个节点（节点自己可以有备份），不必每个节点都存储所有数据。
   
   一致性 Hash 算法将整个 Hash 值空间组织成一个虚拟的圆环，整个空间按照顺时针方向组织，下一步将各个 master 节点（使用服务器的 ip 或主机名）进行 Hash. 这样就能确定每个节点在哈西环上的位置。
   
   来一个 Key，首先计算 Hash 值，并确定此数据在环上的位置，从此位置沿顺时针行走，遇到的第一个 master 节点就是 key 所在的位置。
   
   在一致性 Hash 算法中，如果一个节点挂了，受影响的数据仅仅是从此节点到环空间前一个节点（沿着环逆时针方向遇到的第一个节点）之间的数据，其他不受影响。增加一个节点同理。
   
   但是一致性 Hash 算法在节点太少时，容易因为节点分布不均匀而造成缓存热点的问题。为了解决缓存热点的问题，一致性 Hash 算法引入了虚拟节点机制。即将每台物理服务器虚拟为一组虚拟服务器，将虚拟服务器放置到 Hash 环上，如果要确定对象的服务器，需要先确定对象所在的虚拟服务器，再由虚拟服务器确定物理服务器。
   
   <img src="https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202112202132823.png" alt="image-20210515171539693" style="zoom:50%;" />

3. Hash Slot 算法
   
   Redis Cluster 有固定的 16384 个 Hash Slot，对每个 Key 计算 CRC16 值，然后对 16384 取模，可以获取 Key 对应的 Hash Slot。
   
   每个服务器节点可以负责维护一部分 Slot 以及 Slot 所映射的键值数据。
   
   这样的结构很容易添加或者是删除节点，如果增加一个节点 6 ，就需要从节点 1-5 获得部分 Slot 分配到节点 6 上。如果想移除节点 1，需要将节点 1 中的 Slot 移到节点 2-5 上，然后将没有任何 Slot 的节点1 从集群中删除即可。
   
   **缓存 Key 的 Hash 结果是和 Slot 绑定的，而不是和服务器节点绑定的，所以节点的更替只需要迁移 Slot 即可平滑过度。**

### 数据库和缓存的双写一致性问题

#### 如何保证缓存和数据库的一致性？

1. **不更新缓存，而是删除缓存。**

Reason 1: 线程安全角度

```
线程 A 更新了数据库
线程 B 更新了数据库
线程 B 更新了缓存
线程 A 更新了缓存
```

正常应该 A 比 B 先更新缓存才对，由于 B 比 A 先更新缓存导致了脏数据。

Reason 2: 业务场景角度

```
1、若是写数据多，读数据少的场景，避免频繁更新，浪费性能。
2、缓存中的值不一定是写入数据库的值，而是经过一系列复杂处理后的值，频繁更新会浪费性能。
```

2. **先操作数据库再操作缓存**（考虑的方向：如果出现不一致，谁先做对业务的影响小，就谁先执行）

先删缓存，再更新数据库：

```
请求 A 进行写操作，删除缓存
请求 B 发现缓存不存在
请求 B 从数据库查询旧值
请求 B 将旧值写入缓存
请求 A 将新值写入数据库
```

如不设置缓存过期时间，会永远是脏数据。

先更新数据库，再删缓存

```
缓存失效
请求 A 从数据库中获取旧值
请求 B 将新值写入数据库
请求 B 删除缓存
请求 A 将查到的旧值写入缓存
```

这种情况会产生脏数据。但是由于读操作的速度远大于写操作，因此上述过程发生概率很小。

3. **更好的策略：延时双删**

```
先淘汰缓存
再写数据库
休眠 1s，再次淘汰缓存
```

这样做可以将1s 内所造成的缓存脏数据再次删除。**具体休眠时间需评估业务后确定。**

4. **删除失败怎么办？重试机制**

<img src="https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202112111150142.png" alt="image-20211211115024979" style="zoom:50%;" />

### 分布式锁

分布式锁是为了在分布式系统中，实现不同线程对代码和资源的同步访问。可以基于数据库（如 MySQL）或者基于缓存（如 Redis）实现分布式锁。

#### Redis 做分布式锁的四大关键点：

- **原子性**：redis.set(key, value) 和 redis.expire(key, time , unit)。是两个原子操作，在加锁成功后，服务若出现宕机（即没有设置过期时间），会造成资源的死锁。

- **过期时间**：不带过期时间会造成死锁，所以它是有效避免死锁的一种手段。

- **锁续期**：**为什么续期？**假设锁过期时间是 3s ，但是业务代码执行了 4s 还没执行完，锁过期被释放了，其他线程请求加锁，此时就会出现 2 个客户端并发执行。
  
  **解决**：开辟另外一个线程，专门用于锁续期，上锁的时候就启一个死循环做续期。核心流程就是判断锁的时间超过了三分之一就进行续期。

- **释放锁**：线程 2 不应该能够释放线程 1 持有的锁，所以锁中设置的 **value** 应能唯一确定一个连接。

#### 基于 Redis 的 setnx() 和 expire() 做分布式锁 （遗弃）

**setnx()**: setnx 的含义就是 SET if NOT EXISTS，其中有两个参数 setnx(key, value)。该方法是原子的，如果 key 不存在，则设置当前 key 成功，返回 1；如果当前 key 已经存在，则设置当前 key 失败，返回 0。

**expire()**: expire() 设置过期时间，setnx() 设置的 key 的超时时间需要由 expire() 来设置。

**使用步骤**：

1. setnx(lock_key, 1) 如果返回 0，则说明占位失败；如果返回 1，则说明占位成功。
2. expire() 命令对 lock_key 设置超时时间，为的是避免死锁问题。
3. 执行完业务代码后，可以通过 delete 命令删除 key。

这个方案有些问题，比如，**如果在第一步 setnx 执行成功后，在 expire 执行成功之前，发生了宕机的现象，那么就依然会出现死锁的问题。**下面方案是对其进行的完善。

#### 基于 Redis 的 setnx()、get()、getset() 方法做分布式锁

这个方案的背景主要是在 setnx() 和 expire() 的方案上针对可能存在的死锁问题，做了一些优化。

**getset**: 这个命令有两个参数 getset(key, newValue)。该方法是原子的，对 key 设置 newValue 这个值，并且返回 key 原来的旧值。假设 key 原来是不存在的，那么多次执行这个命令，会出现下边的效果：

1. getset(key, "value1") 返回 null ，此时 key 的值会被设置为 value1
2. getset(key, "value2") 返回 value1 此时 key 的值会被设置为 value2
3. 依此类推！

**使用步骤**

1. setnx(lock_key, 当前时间 + 过期超时时间)，如果返回 1，则获取锁成功；如果返回 0 则没有获取到锁。
2. get(lock_key) 获取值 oldExpireTime，并将这个 value 值与当前的系统时间进行比较，如果小于当前系统时间，则认为这个锁已经超时，可以允许别的请求重新获取。
3. 计算 newExpireTime = 当前时间 + 过期超时时间，然后 getset(lock_key, newExpireTime) 会返回当前 lock_key 的值 currentExpireTime。
4. 判断 currentExpireTime 和 oldExpireTime 是否相等，如果相等，说明当前 getset 设置成功，获取到了锁。如果不相等，说明这个锁又被别的请求获取走了，那么当前请求可以直接返回失败，或者继续重试。
5. 在获取到锁之后，当前线程可以开始自己的业务处理，当处理完毕后，比较自己的处理时间和对于锁设置的超时时间，如果小于锁设置的超时时间，则直接执行 delete 释放锁；如果大于锁设置的超时时间，则不需要再对锁进行处理。

#### Redis 官方推荐的单实例中实现分布式锁的正确方式（原子性非常重要）

1. 设置锁时候，使用 set 命令，因为其中包含了 setnx, expire 的功能，并且有原子操作的效果，给 key 设置随机值，并且只有在 key 不存在时才设置成功返回 True，并且设置 key 的过期时间（最好使用毫秒）
   
   ```
   SET key_name my_random_value NX PX 30000
   ```

2. 在获取锁并完成相关业务后，需要删除自己设置的锁（必须是只能删除自己设置的锁，不能删除他人设置的锁），**删除原因**：保证服务器资源的高利用效率，不用等到锁过期自动删除。**删除方法**：最好使用 Lua 脚本删除（ Redis 保证执行脚本时不执行其他操作，保证操作的原子性）。

#### Redis 多节点实现的分布式锁（RedLock）：有效防止单点故障

假设有 5 个完全独立的 Redis 主服务器。

1. 获取当前时间戳

2. Client 尝试按照顺序使用相同的 Key, value 获取所有 Redis 服务的锁。在获取锁的过程中，获取时间比锁过期时间短很多（这样为了不要过长时间等待已经关闭的 Redis 服务，并且试着获取下一个 Redis 实例）
   
   比如：TTL 为 5s，设置获取锁最多用 1s，所以如果 1s 内无法获取到锁，就放弃获取这个锁，从而尝试获取下个锁。

3. Client 获取所有能获取到的锁后的时间，减去第一步的时间，这个时间差要小于 TTL 时间，并且至少有 3 个 Redis 实例成功获取锁，才能真正的获取锁成功。

4. 如果成功获取锁，则锁的真正有效时间是 TTL 减去第三步的时间差 的时间。
   
   比如：TTL 是 5s，获取所有锁用了 2s，则真正锁有效时间为 3s（其实应该再减去时钟飘移）

5. 如果客户端由于某些原因获取锁失败，便会开始解锁所有 Redis 实例，因为可能已经获了小于 3 个锁，必须释放，否则影响其他 Client 获取锁。

### 一条 SQL 查询语句时如何执行的？

<img src="https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202111141111087.png" alt="image-20210526111624715" style="zoom:50%;" />

MySQL 可以分为 Server 层和存储引擎两部分。

**Server 层**包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。

**存储引擎层**负责数据的存储和提取。

#### 连接器

第一步，连接器使得我们可以连接到数据库上。连接器负责与客户端建立连接、获取权限、维持和管理连接。

- 如果用户名密码认证通过，连接器会到权限表里面查处你拥有的权限。之后，**这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。**

这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。

#### 查询缓存

MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果会以 key-value 对的形式，被直接缓存在内存中。如果查询能够直接在这个缓存中找到 key ，那么这个 value 就会直接返回客户端。

**因为查询往往弊大于利， 所以大多数情况还是建议不要使用查询缓存。**查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。

#### 分析器

对输入的内容的词法、语法进行分析，这样才能知道写的 SQL 语句是要做什么。

#### 优化器

**优化器是在表里面有多个索引，决定使用哪个索引；或者在一个语句有多表关联的时候，决定各个表的连接顺序。**

![image-20211114231042011](https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202111142310054.png)      

这两种执行方法的逻辑结果是一样的，但是执行效率会有所不同，而优化器的作用就是决定选择使用哪一个方案。

优化器阶段完成后，这个语句的执行方案就确定下来了。

#### 执行器

MySQL 通过分析器知道了要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。

开始执行的时候，要先判断是否有这个表的查询权限，如果没有，就会返回没有权限的错误。

如果有权限，就打开表继续执行。打开表的时候，执行器会根据表的引擎定义，去使用这个引擎提供的接口。

### 一条 SQL更新语句是如何执行的？

**一条查询语句的执行过程一般是经过连接器、分析器、优化器、执行器等功能模块，最后到达存储引擎。**

对于查询语句的一套流程，更新语句同样也会走一遍。但是与查询流程不一样的是，更新流程还涉及两个重要的日志模块：redo log （重做日志）和 binlog （归档日志）。

#### 重要的日志模块 redo log

如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了解决这个问题， MySQL 里使用了 WAL 技术，全称 Write-Ahead-Logging，它的关键点是先写日志，再写磁盘。

具体来说，当有一条记录需要更新的时候， InnoDB 引擎就会先把记录写到 redo log 里面，并更新内存，这个时候更新就算完成了。同时， InnoDB 引擎会在适当的时候，将这个操作更新到磁盘里面。

InnoDB 的 redo log 是固定大小的，比如可以配置一组 4 个文件，每个文件的大小是 1GB ，那个就可以操作 4GB 的日志文件。从头开始写，写到末尾回到开头循环写，如下图所示：

![img](https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202111152236980.png)

write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。

write pos 和 checkpoint 之间的是 redo log 日志上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示 redo log 日志满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。

有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 **crash-safe**。

#### 重要的日志文件 binlog

MySQL 从整体来看，分为两块：一块是 Server 层， 主要做的事 MySQL 功能层面的事情。还有一块是引擎层，主要负责数据的存取。 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，就是 binlog 。

**redo log 和 binlog 日志的不同：**

1. redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
2. redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 id = 2 这一行的 c 字段加 1”。
3. redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。

执行器和 InnoDB 引擎在执行简单的 update 语句时的内部流程为（省去了写 undo log 的过程）：

1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。
3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。
4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。
5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。

所以 update 语句的执行流程图如下：

![img](https://raw.githubusercontent.com/Gaytohub/notesPic/main/imgNotes/202111172058841.png)

最后三步有一些绕，将 redo log 的写入拆成了两个步骤：prepare 和 commit ，这就是**“两阶段提交”**。

#### 两阶段提交

首先回答一个问题：**怎样让数据库恢复到半个月内任意一秒的状态？**

前提条件：备份系统一定会保存最近半个月所有的 binlog，同时系统会定期做整库备份。

假设某天下午两点发现中午十二点有一次误删表，需要找回数据，那么可以这么做：

- 首先，找到离中午十二点最近对一次全量备份。
- 然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。

这样临时库就跟误删之前的线上库一样了，然后可以把表数据从临时表取出来，按需要恢复到线上库去。

**如果不使用两阶段提交会出现什么问题呢？**

数据库的状态有可能和用它的日志恢复出来的库的状态不一致。假设将数据库中某行的 c 字段 从 0 修改为 1。在没有使用两阶段提交的情况下：

1. **先写 redo log 后写 binlog** ：redo log ✓  binlog ×  。系统奔溃后重启，由于 redo log 写成功，数据可以恢复过来，即 c = 1. 但是 binlog 没有记录这句话，如果需要使用 binlog 恢复临时库的话，恢复出来的 c 的值为 0，与原库不同。
2. **先写 bin log 后写 redo log** ：binlog ✓ redo log ×  。系统奔溃后重启，由于 redo log 并没有记录该变化日志，所以数据库中 c 值为 0. 但是 binlog 中记录了该日志。如果需要使用 binlog 恢复临时库的话，恢复出来的 c 的值为 1，与原库不同。

#### binlog 日志的格式

一共有三种 ： statement ，row 和 mixed。 其中 statement 格式记录的是执行的完整 SQL 语句。 row 格式记录的执行 SQL 语句所影响到的行信息。mixed 格式则是 statement 和 row 两种格式的混合。

**为什么会有 mixed 格式存在？** 这是因为其他两种格式都存在一定的问题，1、在 statement 格式下，SQL 语句若是存在 LIMIT ，则备库读取 binlog 的时候，可能由于选择的索引不一样，导致 LIMIT 后所取到的行不一样，造成主备不一致。  2、row 格式下，如果某 SQL 语句涉及行数很多，则 binlog 会记录过多的日志，造成 IO 压力大。

但现在还是 row 格式使用的比较多，因为使用 row 格式能够方便数据的恢复。（INSERT ---> DELETE, UPDATE ---> UPDATE)
